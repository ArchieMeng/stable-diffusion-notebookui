{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFsUePWpKOec"
   },
   "source": [
    "# Stable Diffusion Notebook UI\n",
    "Main workflow reference: https://medium.com/@natsunoyuki/using-civitai-models-with-diffusers-package-45e0c475a67e\n",
    "\n",
    "Notebook reference: https://github.com/woctezuma/stable-diffusion-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufD_d64nr08H"
   },
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade diffusers transformers mediapy compel accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t123ZHCrseRr"
   },
   "outputs": [],
   "source": [
    "# restart to use newly installed packages\n",
    "restart = False # @param {type:\"boolean\"}\n",
    "# @markdown > This is usually necessary on colab.\n",
    "if restart:\n",
    "    import os\n",
    "    os._exit(0)  # Restart the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "G_LHyoMTMFAY"
   },
   "outputs": [],
   "source": [
    "# @markdown ### **Fetch model file**\n",
    "import os\n",
    "\n",
    "\n",
    "mode = \"http\" # @param ['http', 'gdrive', 'git', 'local']\n",
    "location = \"https://huggingface.co/mirroring/civitai_mirror/resolve/main/models/Stable-diffusion/AnythingV5V3_v5PrtRE.safetensors\" # @param {type:\"string\"}\n",
    "model_filename = location.split('/')[-1]\n",
    "match mode:\n",
    "  case \"http\":\n",
    "    !test -f $model_filename || wget $location\n",
    "  case \"gdrive\":\n",
    "    from google.colab import drive\n",
    "\n",
    "    try:\n",
    "       drive_path = \"/content/drive\"\n",
    "       drive.mount(drive_path,force_remount=False)\n",
    "    except:\n",
    "       print(\"...error mounting drive or with drive path variables\")\n",
    "  case \"git\":\n",
    "    !git clone --recursive $location\n",
    "  case _:\n",
    "    # default to local mode\n",
    "    pass\n",
    "\n",
    "if model_filename.endswith(\".safetensors\"):\n",
    "  !test -f convert_original_stable_diffusion_to_diffusers.py || wget https://raw.githubusercontent.com/huggingface/diffusers/v$(pip show diffusers | grep Version | awk '{print $2}')/scripts/convert_original_stable_diffusion_to_diffusers.py\n",
    "  model_dir = model_filename.split('.')[0]\n",
    "  if mode == \"http\":\n",
    "    location = model_filename\n",
    "  !python convert_original_stable_diffusion_to_diffusers.py \\\n",
    "      --checkpoint_path $location \\\n",
    "      --dump_path $model_dir/ \\\n",
    "      --from_safetensors\n",
    "elif mode != \"http\" and os.path.isdir(location):\n",
    "  # use model directory directly\n",
    "  model_dir = location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "bG2hkmSEvByV"
   },
   "outputs": [],
   "source": [
    "import mediapy as media\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "import transformers\n",
    "\n",
    "# @markdown #### Pipeline Settings\n",
    "\n",
    "clip_skip = 2 # @param {type:\"slider\", min:1, max:12, step:1}\n",
    "# @markdown > Clip skip = 1 uses the all text encoder layers.\n",
    "# @markdown >\n",
    "# @markdown > Clip skip = 2 skips the last text encoder layer.\n",
    "force_cpu_processing = False # @param {type:\"boolean\"}\n",
    "# @markdown > force using CPU device for interference\n",
    "force_fp32_processing = False # @param {type:\"boolean\"}\n",
    "# @markdown > force float32 processing format. Usually needed when image results are black.\n",
    "use_accelerate = False\n",
    "# @markdown > Use accelerate to fully utilize all GPUs.\n",
    "\n",
    "torch.set_num_threads(os.cpu_count())\n",
    "if use_accelerate:\n",
    "    # Avoids initializing CUDA before processing\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch_dtype = torch.float16\n",
    "elif not force_cpu_processing and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch_dtype = torch.float32\n",
    "\n",
    "if force_fp32_processing:\n",
    "  torch_dtype = torch.float32\n",
    "\n",
    "# clean pipeline if exists\n",
    "if \"pipe\" in locals():\n",
    "  del pipe\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "pipeline_kwargs = dict()\n",
    "if clip_skip > 2:\n",
    "  pipeline_kwargs['text_encoder'] = transformers.CLIPTextModel.from_pretrained(\n",
    "      \"runwayml/stable-diffusion-v1-5\",\n",
    "      subfolder = \"text_encoder\",\n",
    "      num_hidden_layers = 12 - (clip_skip - 1),\n",
    "      torch_dtype = torch_dtype\n",
    "  )\n",
    "\n",
    "if not use_accelerate:\n",
    "    # Use late pipeline initializations for accelerate\n",
    "    kwargs = dict()\n",
    "    if clip_skip > 2:\n",
    "        kwargs['text_encoder'] = transformers.CLIPTextModel.from_pretrained(\n",
    "          \"runwayml/stable-diffusion-v1-5\",\n",
    "          subfolder = \"text_encoder\",\n",
    "          num_hidden_layers = 12 - (clip_skip - 1),\n",
    "          torch_dtype = torch_dtype\n",
    "        )\n",
    "    pipe = DiffusionPipeline.from_pretrained(\n",
    "        model_dir,\n",
    "        safety_checker = None,\n",
    "        torch_dtype = torch_dtype,\n",
    "        **pipeline_kwargs,\n",
    "      ).to(device)\n",
    "\n",
    "    if device.type != 'cpu':\n",
    "        pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "M4Ix-a_hZWFT"
   },
   "outputs": [],
   "source": [
    "# @markdown # Prepare utils functions\n",
    "# Prompt embeddings to overcome CLIP 77 token limit.\n",
    "# https://github.com/huggingface/diffusers/issues/2136\n",
    "# https://huggingface.co/docs/diffusers/using-diffusers/weighted_prompts\n",
    "from compel import Compel, ReturnedEmbeddingsType\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from accelerate import Accelerator\n",
    "\n",
    "\n",
    "def get_prompt_embeddings(\n",
    "    pipeline,\n",
    "    prompt,\n",
    "    negative_prompt,\n",
    "    device = torch.device(\"cpu\")\n",
    "):\n",
    "    if isinstance(pipeline, StableDiffusionXLPipeline):\n",
    "        compel = Compel(\n",
    "          tokenizer=[pipeline.tokenizer, pipeline.tokenizer_2] ,\n",
    "          text_encoder=[pipeline.text_encoder, pipeline.text_encoder_2],\n",
    "          truncate_long_prompts=False,\n",
    "          returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
    "          requires_pooled=[False, True],\n",
    "          device=device\n",
    "        )\n",
    "        prompt_embeds, pooled_prompt_embeds = compel(prompt)\n",
    "        negative_prompt_embeds, negative_pooled_prompt_embeds = compel(negative_prompt)\n",
    "    else:\n",
    "        compel = Compel(\n",
    "          tokenizer=pipeline.tokenizer ,\n",
    "          text_encoder=pipeline.text_encoder,\n",
    "          truncate_long_prompts=False,\n",
    "          returned_embeddings_type = ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NORMALIZED if clip_skip > 1 \\\n",
    "            else ReturnedEmbeddingsType.LAST_HIDDEN_STATES_NORMALIZED,\n",
    "          requires_pooled=False,\n",
    "          device=device\n",
    "        )\n",
    "        prompt_embeds, pooled_prompt_embeds = compel(prompt), None\n",
    "        negative_prompt_embeds, negative_pooled_prompt_embeds = compel(negative_prompt), None\n",
    "\n",
    "    prompt_embeds, negative_prompt_embeds = compel.pad_conditioning_tensors_to_same_length([prompt_embeds, negative_prompt_embeds])\n",
    "    return prompt_embeds, pooled_prompt_embeds, negative_prompt_embeds, negative_pooled_prompt_embeds\n",
    "\n",
    "def accelerated_generate(h, w, cfg_scale, num_inference_steps, batch_size, seeds, kwargs, result_list, use_prompt_embeddings):\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    \n",
    "    pipe = DiffusionPipeline.from_pretrained(\n",
    "        model_dir,\n",
    "        safety_checker = None,\n",
    "        torch_dtype = torch_dtype,\n",
    "        **pipeline_kwargs,\n",
    "      ).to(device)\n",
    "    pipe.to(device)\n",
    "    \n",
    "    if use_prompt_embeddings:\n",
    "        prompt_embeds, pooled_prompt_embeds, negative_prompt_embeds, negative_pooled_prompt_embeds = get_prompt_embeddings(\n",
    "            pipe,\n",
    "            kwargs.pop('prompt'),\n",
    "            kwargs.pop('negative_prompt'),\n",
    "            device = device\n",
    "        )\n",
    "        if isinstance(pipe, StableDiffusionXLPipeline):\n",
    "            kwargs += {\n",
    "                'pooled_prompt_embeds': pooled_prompt_embeds,\n",
    "                'negative_pooled_prompt_embeds': negative_pooled_prompt_embeds,\n",
    "            }\n",
    "        kwargs['prompt_embeds'] = prompt_embeds\n",
    "        kwargs['negative_prompt_embeds'] = negative_prompt_embeds\n",
    "    else:\n",
    "        kwargs['prompt'] = prompt\n",
    "        kwargs['negative_prompt'] = negative_prompt\n",
    "    \n",
    "    with accelerator.split_between_processes(seeds) as task_seeds:\n",
    "        images = pipe(\n",
    "                height = h,\n",
    "                width = w,\n",
    "                guidance_scale = cfg_scale,\n",
    "                num_inference_steps = num_inference_steps,\n",
    "                num_images_per_prompt = len(task_seeds),\n",
    "                generator = [torch.Generator(device).manual_seed(s) for s in task_seeds],\n",
    "                **kwargs,\n",
    "            ).images\n",
    "        result_list.extend(zip(images, task_seeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AUc4QJfE-uR9"
   },
   "outputs": [],
   "source": [
    "# @markdown ### **Prompts**\n",
    "# @markdown > Default parameters are from image https://civitai.com/images/963361\n",
    "prompt = \"bubble, rating:safe, air_bubble, underwater, 1girl, fish, long_hair, submerged, school_uniform, serafuku, solo, water, skirt, neckerchief, short_sleeves,(Impressionism:1.4),\" # @param {type:\"string\"}\n",
    "negative_prompt = \"(mutated hands and fingers:1.5 ),(mutation, poorly drawn :1.2),(long body :1.3),(mutation, poorly drawn :1.2),liquid body,text font ui,long neck,uncoordinated body,fused ears,(ugly:1.4),one hand with more than 5 fingers,one hand with less than 5 fingers,\" # @param {type:\"string\"}\n",
    "use_prompt_embeddings = True # @param {type:\"boolean\"}\n",
    "# @markdown > Prompt embeddings: Overcome CLIP 77 tokens limit.\n",
    "\n",
    "# @markdown ### **Image settings**\n",
    "w = 640 # @param {type:\"slider\", min:64, max:2048, step:8}\n",
    "h = 960 # @param {type:\"slider\", min:64, max:2048, step:8}\n",
    "batch_count = 1 # @param {type:\"integer\"}\n",
    "batch_size = 1 # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown ### **Generation settings**\n",
    "cfg_scale = 9 # @param {type:\"slider\", min:1, max:27, step:0.5}\n",
    "num_inference_steps = 30 # @param {type:\"slider\", min:1, max:150, step:1}\n",
    "\n",
    "init_seed = 1678803042 # @param {type: \"integer\"}\n",
    "#  @markdown > seed: set -1 for random seed\n",
    "\n",
    "# Arguments preparation\n",
    "results = []\n",
    "seeds = [\n",
    "    random.randint(0, sys.maxsize) if init_seed == -1\n",
    "    else init_seed + i\n",
    "    for i in range(batch_count * batch_size)\n",
    "]\n",
    "# round image size to be divisible by 8\n",
    "w -= w % 8\n",
    "h -= h % 8\n",
    "\n",
    "kwargs = dict()\n",
    "if use_prompt_embeddings and not use_accelerate:\n",
    "    # for accelerate, embedding should be generated in forked processes with accelerate devices.\n",
    "    prompt_embeds, pooled_prompt_embeds, negative_prompt_embeds, negative_pooled_prompt_embeds = get_prompt_embeddings(\n",
    "      pipe,\n",
    "      prompt,\n",
    "      negative_prompt,\n",
    "      device = device\n",
    "    )\n",
    "    if isinstance(pipe, StableDiffusionXLPipeline):\n",
    "        kwargs = {\n",
    "            'pooled_prompt_embeds': pooled_prompt_embeds,\n",
    "            'negative_pooled_prompt_embeds': negative_pooled_prompt_embeds,\n",
    "        }\n",
    "    kwargs['prompt_embeds'] = prompt_embeds\n",
    "    kwargs['negative_prompt_embeds'] = negative_prompt_embeds\n",
    "else:\n",
    "    kwargs['prompt'] = prompt\n",
    "    kwargs['negative_prompt'] = negative_prompt\n",
    "\n",
    "for i in range(batch_count):\n",
    "    batched_seeds = seeds[i * batch_size: (i + 1) * batch_size]\n",
    "    if use_accelerate:\n",
    "        from accelerate import notebook_launcher\n",
    "        from multiprocessing import Manager\n",
    "        \n",
    "        with Manager() as manager:\n",
    "            result_list = manager.list()\n",
    "            notebook_launcher(\n",
    "                accelerated_generate, \n",
    "                (h, w, cfg_scale, num_inference_steps, batch_size, batched_seeds, kwargs, result_list, use_prompt_embeddings,), \n",
    "                num_processes=torch.cuda.device_count()\n",
    "            )\n",
    "            results.extend(result_list)\n",
    "            # Todo: may produce out of order image list\n",
    "            # Todo: use fp16 with accelerate\n",
    "    else:\n",
    "        images = pipe(\n",
    "            height = h, \n",
    "            width = w,\n",
    "            guidance_scale = cfg_scale,\n",
    "            num_inference_steps = num_inference_steps,\n",
    "            num_images_per_prompt = batch_size,\n",
    "            generator = [torch.Generator(device).manual_seed(s) for s in batched_seeds],\n",
    "            **kwargs,\n",
    "        ).images\n",
    "        results.extend(zip(images, batched_seeds))\n",
    "\n",
    "media.show_images([r[0] for r in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "siDk-W0AKOeg"
   },
   "outputs": [],
   "source": [
    "# @markdown Save Images with seeds\n",
    "!mkdir -pv outputs\n",
    "\n",
    "for img, seed in results:\n",
    "    filename = f\"outputs/{seed}.png\"\n",
    "    img.save(filename)\n",
    "\n",
    "!zip -rv -9 outputs.zip outputs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
