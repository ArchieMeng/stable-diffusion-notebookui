{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFsUePWpKOec"
   },
   "source": [
    "# Stable Diffusion Notebook UI\n",
    "Main workflow reference: https://medium.com/@natsunoyuki/using-civitai-models-with-diffusers-package-45e0c475a67e\n",
    "\n",
    "Notebook reference: https://github.com/woctezuma/stable-diffusion-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufD_d64nr08H"
   },
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade diffusers transformers mediapy compel accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t123ZHCrseRr"
   },
   "outputs": [],
   "source": [
    "# restart to use newly installed packages\n",
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "G_LHyoMTMFAY"
   },
   "outputs": [],
   "source": [
    "# @markdown ### **Fetch model file**\n",
    "import os\n",
    "\n",
    "\n",
    "mode = \"http\" # @param ['http', 'gdrive', 'git']\n",
    "location = \"https://huggingface.co/mirroring/civitai_mirror/resolve/main/models/Stable-diffusion/AnythingV5V3_v5PrtRE.safetensors\" # @param {type:\"string\"}\n",
    "model_filename = location.split('/')[-1]\n",
    "match mode:\n",
    "  case \"http\":\n",
    "    !test -f $model_filename || wget $location\n",
    "  case \"gdrive\":\n",
    "    from google.colab import drive\n",
    "\n",
    "    try:\n",
    "       drive_path = \"/content/drive\"\n",
    "       drive.mount(drive_path,force_remount=False)\n",
    "    except:\n",
    "       print(\"...error mounting drive or with drive path variables\")\n",
    "  case \"git\":\n",
    "    !git clone --recursive $location\n",
    "  case _:\n",
    "    # default to local mode\n",
    "    pass\n",
    "\n",
    "if model_filename.endswith(\".safetensors\"):\n",
    "  !test -f convert_original_stable_diffusion_to_diffusers.py || wget https://raw.githubusercontent.com/huggingface/diffusers/v$(pip show diffusers | grep Version | awk '{print $2}')/scripts/convert_original_stable_diffusion_to_diffusers.py\n",
    "  model_dir = model_filename.split('.')[0]\n",
    "  if mode == \"http\":\n",
    "    location = model_filename\n",
    "  !python convert_original_stable_diffusion_to_diffusers.py \\\n",
    "      --checkpoint_path $location \\\n",
    "      --dump_path $model_dir/ \\\n",
    "      --from_safetensors\n",
    "elif mode != \"http\" and os.path.isdir(location):\n",
    "  # use model directory directly\n",
    "  model_dir = location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "bG2hkmSEvByV"
   },
   "outputs": [],
   "source": [
    "import mediapy as media\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "import transformers\n",
    "\n",
    "# @markdown #### Pipeline Settings\n",
    "\n",
    "clip_skip = 2 # @param {type:\"slider\", min:1, max:12, step:1}\n",
    "# @markdown > Clip skip = 1 uses the all text encoder layers.\n",
    "# @markdown >\n",
    "# @markdown > Clip skip = 2 skips the last text encoder layer.\n",
    "force_cpu_processing = False # @param {type:\"boolean\"}\n",
    "# @markdown > force using CPU device for interference\n",
    "force_fp32_processing = False # @param {type:\"boolean\"}\n",
    "# @markdown > force float32 processing format. Usually needed when image results are black.\n",
    "\n",
    "torch.set_num_threads(os.cpu_count())\n",
    "if not force_cpu_processing and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch_dtype = torch.float32\n",
    "\n",
    "if force_fp32_processing:\n",
    "  torch_dtype = torch.float32\n",
    "\n",
    "# clean pipeline if exists\n",
    "if \"pipe\" in locals():\n",
    "  del pipe\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "kwargs = dict()\n",
    "if clip_skip > 2:\n",
    "  kwargs['text_encoder'] = transformers.CLIPTextModel.from_pretrained(\n",
    "      \"runwayml/stable-diffusion-v1-5\",\n",
    "      subfolder = \"text_encoder\",\n",
    "      num_hidden_layers = 12 - (clip_skip - 1),\n",
    "      torch_dtype = torch_dtype\n",
    "  )\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    model_dir,\n",
    "    safety_checker = None,\n",
    "    torch_dtype = torch_dtype,\n",
    "    **kwargs,\n",
    "  ).to(device)\n",
    "\n",
    "if device.type != 'cpu':\n",
    "    pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "M4Ix-a_hZWFT"
   },
   "outputs": [],
   "source": [
    "# @markdown # Prepare utils functions\n",
    "# Prompt embeddings to overcome CLIP 77 token limit.\n",
    "# https://github.com/huggingface/diffusers/issues/2136\n",
    "# https://huggingface.co/docs/diffusers/using-diffusers/weighted_prompts\n",
    "from compel import Compel, ReturnedEmbeddingsType\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "\n",
    "def get_prompt_embeddings(\n",
    "    pipeline,\n",
    "    prompt,\n",
    "    negative_prompt,\n",
    "    device = torch.device(\"cpu\")\n",
    "):\n",
    "    if isinstance(pipe, StableDiffusionXLPipeline):\n",
    "        compel = Compel(\n",
    "          tokenizer=[pipeline.tokenizer, pipeline.tokenizer_2] ,\n",
    "          text_encoder=[pipeline.text_encoder, pipeline.text_encoder_2],\n",
    "          truncate_long_prompts=False,\n",
    "          returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
    "          requires_pooled=[False, True],\n",
    "          device=device\n",
    "        )\n",
    "        prompt_embeds, pooled_prompt_embeds = compel(prompt)\n",
    "        negative_prompt_embeds, negative_pooled_prompt_embeds = compel(negative_prompt)\n",
    "    else:\n",
    "        compel = Compel(\n",
    "          tokenizer=pipeline.tokenizer ,\n",
    "          text_encoder=pipeline.text_encoder,\n",
    "          truncate_long_prompts=False,\n",
    "          returned_embeddings_type = ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NORMALIZED if clip_skip > 1 \\\n",
    "            else ReturnedEmbeddingsType.LAST_HIDDEN_STATES_NORMALIZED,\n",
    "          requires_pooled=False,\n",
    "          device=device\n",
    "        )\n",
    "        prompt_embeds, pooled_prompt_embeds = compel(prompt), None\n",
    "        negative_prompt_embeds, negative_pooled_prompt_embeds = compel(negative_prompt), None\n",
    "\n",
    "    prompt_embeds, negative_prompt_embeds = compel.pad_conditioning_tensors_to_same_length([prompt_embeds, negative_prompt_embeds])\n",
    "    return prompt_embeds, pooled_prompt_embeds, negative_prompt_embeds, negative_pooled_prompt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AUc4QJfE-uR9"
   },
   "outputs": [],
   "source": [
    "# @markdown ### **Prompts**\n",
    "# @markdown > Default parameters are from image https://civitai.com/images/963361\n",
    "prompt = \"bubble, rating:safe, air_bubble, underwater, 1girl, fish, long_hair, submerged, school_uniform, serafuku, solo, water, skirt, neckerchief, short_sleeves,(Impressionism:1.4),\" # @param {type:\"string\"}\n",
    "negative_prompt = \"(mutated hands and fingers:1.5 ),(mutation, poorly drawn :1.2),(long body :1.3),(mutation, poorly drawn :1.2),liquid body,text font ui,long neck,uncoordinated body,fused ears,(ugly:1.4),one hand with more than 5 fingers,one hand with less than 5 fingers,\" # @param {type:\"string\"}\n",
    "use_prompt_embeddings = True # @param {type:\"boolean\"}\n",
    "# @markdown > Prompt embeddings: Overcome CLIP 77 tokens limit.\n",
    "\n",
    "# @markdown ### **Image settings**\n",
    "w = 640 # @param {type:\"slider\", min:64, max:2048, step:1}\n",
    "h = 960 # @param {type:\"slider\", min:64, max:2048, step:1}\n",
    "batch_count = 1 # @param {type:\"integer\"}\n",
    "batch_size = 1 # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown ### **Generation settings**\n",
    "cfg_scale = 9 # @param {type:\"slider\", min:1, max:27, step:0.5}\n",
    "num_inference_steps = 30 # @param {type:\"slider\", min:1, max:150, step:1}\n",
    "\n",
    "init_seed = 1678803042 # @param {type: \"integer\"}\n",
    "#  @markdown > seed: set -1 for random seed\n",
    "\n",
    "# Arguments preparation\n",
    "images = []\n",
    "seeds = [\n",
    "    random.randint(0, sys.maxsize) if init_seed == -1\n",
    "    else init_seed + i\n",
    "    for i in range(batch_count * batch_size)\n",
    "]\n",
    "# round image size to be divisible by 8\n",
    "w -= w % 8\n",
    "h -= h % 8\n",
    "\n",
    "kwargs = dict()\n",
    "if use_prompt_embeddings:\n",
    "    prompt_embeds, pooled_prompt_embeds, negative_prompt_embeds, negative_pooled_prompt_embeds = get_prompt_embeddings(\n",
    "      pipe,\n",
    "      prompt,\n",
    "      negative_prompt,\n",
    "      device = device\n",
    "    )\n",
    "    if isinstance(pipe, StableDiffusionXLPipeline):\n",
    "        kwargs = {\n",
    "            'pooled_prompt_embeds': pooled_prompt_embeds,\n",
    "            'negative_pooled_prompt_embeds': negative_pooled_prompt_embeds,\n",
    "        }\n",
    "    kwargs['prompt_embeds'] = prompt_embeds\n",
    "    kwargs['negative_prompt_embeds'] = negative_prompt_embeds\n",
    "else:\n",
    "    kwargs['prompt'] = prompt\n",
    "    kwargs['negative_prompt'] = negative_prompt\n",
    "\n",
    "for i in range(batch_count):\n",
    "    images += pipe(\n",
    "        height = h,\n",
    "        width = w,\n",
    "        guidance_scale = cfg_scale,\n",
    "        num_inference_steps = num_inference_steps,\n",
    "        num_images_per_prompt = batch_size,\n",
    "        generator = [torch.Generator(device).manual_seed(s) for s in seeds[i * batch_size: (i + 1) * batch_size]],\n",
    "        **kwargs,\n",
    "    ).images\n",
    "\n",
    "media.show_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "siDk-W0AKOeg"
   },
   "outputs": [],
   "source": [
    "# @markdown Save Images with seeds\n",
    "!mkdir outputs\n",
    "\n",
    "for seed, img in zip(seeds, images):\n",
    "    filename = f\"outputs/{seed}.png\"\n",
    "    img.save(filename)\n",
    "\n",
    "!zip -rv -9 outputs.zip outputs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
